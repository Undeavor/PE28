{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHsc+K+ycmVbYRUIdBNL2o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["On souhaite optimiser un portefeuille en ayant la pr√©diction des rendements et des volatilit√©s d'un ensemble d'actifs √† n √©l√©ments.\n","\n","On a $\\sum_iw_i=1$.\n","\n","Pour √©tablir la volatilit√© d'un portefeuille de rendement $R_P=\\sum_iw_iR_i$, il est n√©cessaire de connaitre les covariances entre les diff√©rents rendements et donc de l'estimer √† travers d'un mod√®le choisi.\n"],"metadata":{"id":"phUH87OcZEJl"}},{"cell_type":"markdown","source":["# Deux choix d'optimisation sont possibles :    \n","\n","## 1 - Maximisation du Sharpe\n","Soit $w = (w_1,  \\ldots, w_n)^T$ l'ensemble des poids associ√©s aux actifs,\n","$R_p = (R_1, \\ldots, R_n)^{T}$ le vecteur des rendements esp√©r√©s,\n","$\\Sigma$ la matrice de covariance des rendements, et $r_f$ le taux sans risque.\n","\n","Alors le ratio de Sharpe du portefeuille s'√©crit : ( les termes entre parenth√®ses sont les notations matricielles )\n","$$\n","\\text{Sharpe} = \\frac{\\sum_iw_iR_i - R_f}{\\sigma_p}\n","= \\frac{\\sum_iw_i(R_i - R_f)}{\\sqrt{\\sum_i w_i^2\\sigma_i^2+\\sum_{i‚â†j}w_iw_i\\text{cov}(r_i,r_j)}}(= \\frac{w^{T}(R_p - R_f \\mathbf{1})}{\\sqrt{w^{T} \\Sigma w}})\n","$$\n","o√π\n","$\\sum_iw_iR_p - R_f$ est le rendement exc√©dentaire esp√©r√© et\n","$\\sigma_p = \\sqrt{w^{T} \\Sigma w}$ repr√©sente la volatilit√© du portefeuille.\n","\n","L‚Äôobjectif consiste √† maximiser ce ratio sous la contrainte de normalisation :\n","$$\n","\\max_{w} \\frac{\\sum_iw_i(R_i - R_f)}{\\sqrt{\\sum_i w_i^2\\sigma_i^2+\\sum_{i‚â†j}w_iw_i\\text{cov}(r_i,r_j)}}\n","\\quad \\text{avec} \\quad \\sum_i w_i= 1(=\\mathbf{1}^{T}w )\n","$$\n","\n","---\n","\n","## 2 - Minimisation de la volatilit√© pour un rendement cible\n","\n","On cherche ici √† obtenir le portefeuille le moins risqu√© possible\n","pour un rendement esp√©r√© fix√© $R_{t}$ :\n","$$\\min_{w} \\; \\sum_i w_i^2\\sigma_i^2+\\sum_{i‚â†j}w_iw_i\\text{cov}(r_i,r_j) \\quad \\text{avec} \\quad \\begin{cases} \\sum_i w_i= 1(=\\mathbf{1}^{T}w ), \\\\\\ \\sum_iw_iR_i = R_{t}(=R_p^{T}w).\\end{cases}$$\n","\n","---\n","\n","Cette formulation permet de tracer la fronti√®re d'efficience,\n","ensemble des portefeuilles offrant le rendement maximal pour un niveau de risque donn√©\n","ou, inversement, le risque minimal pour un rendement donn√©."],"metadata":{"id":"E9ro1ihfwjcc"}},{"cell_type":"markdown","source":["Hypoth√®ses pour simplifier le mod√®le dans un premier temps :\n","- deux actifs maximum\n","- les titres sont scindables √† souhait\n","- pas de frais\n","- pas de dividendes\n","- les corr√©lations entre les actifs sont nulles ( ici on ne diversifie pas ) : $ \\sqrt{w^{T} \\Sigma w}=\\sqrt{\\sum_i w_i^2\\sigma_i^2}$\n","- pas de contrainte de signe pour les poids ( long/short autoris√©s )"],"metadata":{"id":"WyMUckEpYSQA"}},{"cell_type":"markdown","source":["D√©monstration de l'optimisation du sharpe sous ces hypoth√®ses :\n","\n","$$\\text{Sharpe}=\\frac{\\sum_iw_i(R_i - R_f)}{\\sqrt{\\sum_i w_i^2\\sigma_i^2}}\\text{et} \\sum_i w_i= 1$$\n","On remarque que le sharpe est invariant par homoth√©tie positive, on prend $\\sqrt{\\sum_i w_i^2\\sigma_i^2}=1‚áî\\frac{1}{2}\\sqrt{\\sum_i w_i^2\\sigma_i^2}=\\frac{1}{2}$.\n"," On applique le th√©or√®me de Lagrange,\n"," $$\\Delta\\text{Sharpe}+\\frac{\\lambda}{2}\\Delta(\\sum_i w_i^2\\sigma_i^2-1)=0 \\\\ ‚áî \\forall i \\in [1,n],\\ (R_i-R_f)+\\lambda(w_i\\sigma_i^2)=0\\\\ ‚áî \\forall i \\in [1,n],\\ w_i=\\frac{(R_f-R_i)}{\\lambda\\sigma_i^2}$$\n"," Or l'on souhaite aussi $\\sum_i w_i= 1$ donc l'on normalise le vecteur:\n"," $$w_i=\\frac{\\frac{(R_f-R_i)}{\\lambda\\sigma_i^2}}{\\sum_i\\frac{(R_f-R_i)}{\\lambda\\sigma_i^2}}$$\n"],"metadata":{"id":"PQL_4I9ypbrv"}},{"cell_type":"markdown","source":["Pas encore montr√©:\n","- ( opti sharpe ) La solution analytique (en absence de contraintes de signe sur les poids) est donn√©e par :\n","$$\n","w^{*} = \\frac{\\Sigma^{-1}(\\mu - r_f \\mathbf{1})}\n","{\\mathbf{1}^{T}\\Sigma^{-1}(\\mu - r_f \\mathbf{1})}\n","$$\n","- ( opti volatilit√© avec rendement fixe ) En notant :\n","$$A = \\mathbf{1}^{T}\\Sigma^{-1}\\mathbf{1}, \\quad B = \\mathbf{1}^{T}\\Sigma^{-1}\\mu, \\quad C = \\mu^{T}\\Sigma^{-1}\\mu, \\quad D = AC - B^{2},$$\n","la solution optimale s‚Äô√©crit :\n","$$w(\\mu_t) = \\Sigma^{-1}\\left[ \\frac{C - B\\mu_t}{D}\\mathbf{1} + \\frac{A\\mu_t - B}{D}\\mu\\right]$$\n","\n","  La variance minimale associ√©e √† ce portefeuille est alors :\n","$$\\sigma_p^{2}(\\mu_t) = \\frac{A\\mu_t^{2} - 2B\\mu_t + C}{D}.$$"],"metadata":{"id":"kMRd9uznRZoc"}},{"cell_type":"markdown","source":["# MGARCH(1,1)\n","Ici l'on consid√®re la matrice de covariance $H_t=(cov(r_i,r_j))_{i,j\\in[1,n]}$ -> TH√âOR√àME SPECTRAL : SYM√âTRIQUE R√âELLE DONC DIAGONALISABLE. Tout comme le mod√®le GARCH(1,1), l'on pose une √©quation r√©currente de $H_t$:\n","$$\\boxed{H_t = C + A \\epsilon_{t-1}r_{t-1}^\\top A^\\top + B H_{t-1} B^\\top}$$\n","pour $n=1$, on retrouve GARCH(1,1):\n","$$\\sigma_t=c+a^2r_{t-1}+b^2\\sigma_{t-1}$$\n","Pourquoi les coefficients sont positifs dans le mgarch n=1 ici alors que ce n'etait pas une hypoth√®se pr√©c√©demment ? JSP\n","\n","Quelle valeur initiale de $H_0$ prendre ? $\\lambda I_n$ ou $diag(\\sigma_1, \\ldots, \\sigma_n)$ s√ªrement\n","\n","##Optimisation de A,B,C par MLL: ( tout est toujours conditionnel ici )\n","\n","On pose $R(t)=(r_1(t),\\ldots,r_n(t))^T \\sim N(0,H(t))$ le vecteur gaussien des rendements de densit√© :\n","\n","$$L(R) = \\frac{1}{(2\\pi)^{N/2} |H|^{1/2}}\n","\\exp\\!\\Big(-\\frac{1}{2}< H^{-1} (R - 0), (R - 0) >\\Big)$$\n","On prend le produit scalaire de vecteurs usuel $<M,N>=M^TN$\n","$$L(R) = \\frac{1}{(2\\pi)^{N/2} |H|^{1/2}}\n","\\exp\\!\\Big(-\\frac{1}{2}R^T H^{-1} R\\Big)$$\n","D'o√π l'expression de la vraisemblance avec Bayes:\n","$$L(R(1), \\ldots,R(T))=\\prod_{i=1}^T \\frac{1}{(2\\pi)^{N/2} |H(i)|^{1/2}}\n","\\exp\\!\\Big(-\\frac{1}{2}R^T(i) H^{-1}(i) R(i)\\Big)$$\n","$$\\boxed{LL(R(1), \\ldots,R(T))=\\sum_{i=1}^T -\\frac{N}{2}log(2\\pi)-\\frac{1}{2}log|H(i)|\n","-\\frac{1}{2}R^T(i) H^{-1}(i) R(i)}$$\n","  \n","-On d√©finit le produit scalaire matriciel comme √©tant:\n","$$<A,B>=Tr(A^TB)$$\n","\n","En scalaire, on a:\n","$$df = \\left(\\frac{\\partial f}{\\partial X}\\right)^\\top dX$$\n","\n","Donc les diff√©rencielles matricielles deviennent:\n","$$df = \\operatorname{tr}\\!\\left(\\left(\\frac{\\partial f}{\\partial X}\\right)^\\top dX\\right)$$\n","\n","-De plus,\n","$$(H+dH)(H^{-1}+dH^{-1})=I$$\n","$$‚áî HH^{-1}+HdH^{-1}+dHH^{-1}+dHdH^{-1}=I$$\n","$$‚áî HdH^{-1}+dHH^{-1}+dHdH^{-1}=0$$\n","$$\\text{ avec } dHdH^{-1} \\text{ n√©gligeable devant les autres termes}$$\n","$$‚áî HdH^{-1}=-dHH^{-1}$$\n","$$‚áî dH^{-1}=-H^{-1}dHH^{-1}$$\n","\n","-Encore, si les valeurs propres de H sont les $\\lambda_i$\n","$$d(log(|H|))=d(\\log |H|) = d\\left(\\sum_i \\log \\lambda_i \\right) = \\sum_i \\frac{d\\lambda_i}{\\lambda_i} = \\operatorname{Tr}(H^{-1} dH)$$\n","\n","\n","-De surcro√Æt, soit A vecteur, B matrice\n","$$A^TB^{-1}dB B^{-1} A=\\operatorname{Tr}(A^TB^{-1}dB B^{-1} A)$$ Or $$\\operatorname{Tr}(AB)=\\operatorname{Tr}(BA)$$ Donc $$A^TB^{-1}dB B^{-1} A=\\operatorname{Tr}(B^{-1} AA^TB^{-1}dB)$$\n","-On a finalement,\n","$$dLL=-\\frac{1}{2}d(log(|H(i)|))-\\frac{1}{2}R^T(i)(dH(i)^{-1}) R(i)$$\n","$$‚áîdLL= -\\frac{1}{2}\\operatorname{Tr}(H(i)^{-1} dH(i))-\\frac{1}{2}R^T(i)(-H(i)^{-1}dH(i)H(i)^{-1}) R(i)$$\n","$$‚áîdLL=-\\frac{1}{2}\\operatorname{Tr}(H(i)^{-1} dH(i))+\\frac{1}{2}\\operatorname{Tr}(H(i)^{-1} R(i)R(i)^TH(i)^{-1}dH(i))$$\n","$$‚áí\\boxed{\\frac{\\partial LL}{\\partial H(i)}=-\\frac{1}{2}H(i)^{-1}+\\frac{1}{2}H(i)^{-1} R(i)R(i)^TH(i)^{-1}}$$\n","On peut donc calculer les d√©riv√©es matricielles par rapport √† A, B et C gr√¢ce √† la r√®gle de la chaine: $$\\frac{\\partial LL}{\\partial A}=\\sum_i\\frac{\\partial LL}{\\partial H(i)}\\frac{\\partial H(i)}{\\partial A}$$\n","donc effectuer une descente de gradient.\n","En effet, lorsqu'en scalaire l'on se d√©place du scalaire $œµ‚àáf$, pour les matrices on se d√©place idem.\n"],"metadata":{"id":"VgNg3FH6vPcF"}},{"cell_type":"markdown","source":["Ici la taille des matrices A, B et C est en O(n^2)\n","\n","Il existe d'autres mod√®les pour r√©duire cette borne:\n","- Bekk\n","- Diagonal MGARCH\n","- Scalar BEKK\n","- Diagonal BEKK\n","- DCC-GARCH (Engle 2002)\n","- VECH and DVEC\n","- GO-GARCH (Generalized Orthogonal GARCH ‚Äì van der Weide, 2002)\n","- üîπ 5Ô∏è‚É£ OGARCH (Orthogonal GARCH ‚Äì Alexander, 2001)\n","- Factor-GARCH (Engle, Ng, Shephard)\n","- Copula-GARCH\n","- Asymmetric MGARCH (AG-DCC, ADCC, GJR-BEKK, etc.)\n"],"metadata":{"id":"YcCtuSWh4xjO"}},{"cell_type":"code","source":["from pymvgarch import BEKK\n","import numpy as np\n","\n","# Example data: T x N\n","data = np.random.randn(1000, 3)\n","model = BEKK(data, p=1, q=1)\n","model.fit()"],"metadata":{"id":"W22OWWXKLC1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UhCagJdw57Uh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Annexe : D√©monstration de la densit√© de la loi gaussienne multivari√©e\n","\n","Soit $X \\in \\mathbb{R}^n$ un vecteur al√©atoire gaussien d'esp√©rance\n","$\\mu \\in \\mathbb{R}^n$ et de matrice de covariance\n","$\\Sigma \\in \\mathbb{R}^{n \\times n}$, sym√©trique d√©finie positive.\n","\n","On cherche √† d√©montrer que la densit√© de $X$ est donn√©e par :\n","\n","$$\n","f_X(x) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}}\n","\\exp!\\Big(-\\tfrac{1}{2}(x-\\mu)^\\top \\Sigma^{-1}(x-\\mu)\\Big),\n","\\qquad x \\in \\mathbb{R}^n.\n","$$\n","\n","‚∏ª\n","\n","1. Cas standard\n","\n","Consid√©rons d'abord un vecteur al√©atoire standard\n","$Z \\sim \\mathcal{N}(0, I_n)$.\n","Sa densit√© est :\n","\n","$$\n","f_Z(z) = (2\\pi)^{-n/2} \\exp!\\Big(-\\tfrac{1}{2} z^\\top z\\Big),\n","\\qquad z \\in \\mathbb{R}^n.\n","$$\n","\n","‚∏ª\n","\n","2. Transformation lin√©aire\n","\n","Puisque $\\Sigma$ est sym√©trique d√©finie positive,\n","il existe une matrice inversible L telle que :\n","\n","$$\n","\\Sigma = L L^\\top.\n","$$\n","\n","(On peut choisir L comme la matrice de Cholesky de $\\Sigma$.)\n","\n","D√©finissons maintenant :\n","\n","$$\n","X = \\mu + LZ.\n","$$\n","\n","Alors :\n","\n","$$\n","\\mathbb{E}[X] = \\mu, \\qquad\n","\\text{Cov}(X) = L , \\text{Cov}(Z) , L^\\top = L I_n L^\\top = \\Sigma.\n","$$\n","\n","Ainsi, $X$ est bien un vecteur gaussien de moyenne $\\mu$ et de covariance $\\Sigma$.\n","\n","‚∏ª\n","\n","3. Changement de variables\n","\n","On a le changement de variables :\n","\n","$$\n","z = L^{-1}(x - \\mu),\n","$$\n","\n","dont le jacobien est :\n","\n","$$\n","|\\det(L^{-1})| = |\\det L|^{-1}.\n","$$\n","\n","La densit√© de $X$ est donc :\n","\n","$$\n","f_X(x) = f_Z(L^{-1}(x - \\mu)) , |\\det(L^{-1})|.\n","$$\n","\n","‚∏ª\n","\n","4. Substitution et simplification\n","\n","En rempla√ßant $f_Z$ :\n","\n","$$\n","f_X(x) = (2\\pi)^{-n/2}\n","\\exp!\\Big(-\\tfrac{1}{2}(L^{-1}(x-\\mu))^\\top (L^{-1}(x-\\mu))\\Big)\n",", |\\det(L^{-1})|.\n","$$\n","\n","Or :\n","\n","$$\n","(L^{-1}(x-\\mu))^\\top (L^{-1}(x-\\mu))\n","= (x-\\mu)^\\top (L^{-1})^\\top L^{-1} (x-\\mu)\n","= (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu).\n","$$\n","\n","De plus :\n","\n","$$\n","|\\det(L^{-1})| = |\\det L|^{-1} = |\\Sigma|^{-1/2}.\n","$$\n","\n","‚∏ª\n","\n","5. Forme finale\n","\n","En regroupant les termes, on obtient :\n","\n","$$\n","\\boxed{\n","f_X(x) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}}\n","\\exp!\\Big(-\\tfrac{1}{2}(x-\\mu)^\\top \\Sigma^{-1}(x-\\mu)\\Big)\n","}.\n","$$\n","\n","‚∏ª\n","\n","Remarque\n","\n","Si $\\Sigma$ est seulement semi-d√©finie positive (et non inversible),\n","la densit√© n'existe pas au sens usuel sur $\\mathbb{R}^n$ :\n","la loi est alors concentr√©e sur un sous-espace affine de dimension √©gale au rang de $\\Sigma$."],"metadata":{"id":"rVsiiWT56Gz0"}}]}